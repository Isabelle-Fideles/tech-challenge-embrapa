# ğŸš€ API Embrapa Uva e Vinho - POS Tech MLE

API para coleta estruturada de dados pÃºblicos da **Embrapa Uva e Vinho**, nas abas:

- ğŸ‡ ProduÃ§Ã£o
- ğŸ· Processamento
- ğŸ’° ComercializaÃ§Ã£o
- ğŸŒ ImportaÃ§Ã£o
- ğŸš¢ ExportaÃ§Ã£o

Esses dados servirÃ£o de base para anÃ¡lise e construÃ§Ã£o de modelos de **Machine Learning** no futuro.

---

## ğŸŒ Links RÃ¡pidos

- [API em produÃ§Ã£o (Swagger UI)](https://tech-challenge-embrapa-1.onrender.com/docs#/)
- [RepositÃ³rio no GitHub](https://github.com/Isabelle-Fideles/tech-challenge-embrapa/tree/main/fase1)
- [VÃ­deo de apresentaÃ§Ã£o](https://www.youtube.com/watch?v=GpOifVpCjZE)

---

## ğŸ“ Como a API Resolve o Problema de Acesso aos Dados da Vitibrasil

Atualmente, os dados pÃºblicos sobre `produÃ§Ã£o`, `processamento`, `comercializaÃ§Ã£o`, `importaÃ§Ã£o` e `exportaÃ§Ã£o` de uvas e vinhos estÃ£o disponÃ­veis no site Vitibrasil, mantido pela Embrapa Uva e Vinho.
**PorÃ©m, o site apresenta limitaÃ§Ãµes importantes:**
- NÃ£o existe API oficial para consulta automÃ¡tica ou integraÃ§Ã£o.
- O acesso Ã© apenas manual, via navegaÃ§Ã£o web e download de arquivos.
- O site sofre instabilidades e pode ficar fora do ar, dificultando anÃ¡lises e integraÃ§Ãµes em tempo real.

**Nossa API resolve esse problema ao:**
- Estruturar e padronizar o acesso aos dados da Embrapa via endpoints RESTful.
- Implementar um sistema de fallback/cache local, garantindo disponibilidade mesmo em caso de instabilidade da fonte.
- Facilitar a integraÃ§Ã£o dos dados com dashboards, sistemas de BI e projetos de Machine Learning.

Assim, o projeto transforma dados antes pouco acessÃ­veis e volÃ¡teis em uma base confiÃ¡vel, pronta para consumo por aplicaÃ§Ãµes modernas.

---

## ğŸ”— SumÃ¡rio
- [ğŸ› ï¸ Tecnologias](#-tecnologias)
- [ğŸ—‚ï¸ Estrutura do Projeto](#-estrutura-do-projeto)
- [ğŸ“ˆ Diagramas do Projeto](#-diagramas-do-projeto)
- [ğŸ—ºï¸ Fluxo Detalhado da API](#-fluxo-detalhado-da-api)
- [âš™ï¸ InstalaÃ§Ã£o e ExecuÃ§Ã£o](#-instalaÃ§Ã£o-e-execuÃ§Ã£o)
- [ğŸ” AutenticaÃ§Ã£o](#-autenticaÃ§Ã£o)
- [ğŸ§ª Testes](#-testes)
- [ğŸ” Fluxo de Funcionamento](#-fluxo-de-funcionamento)
- [â­ Diferenciais e Boas PrÃ¡ticas](#-diferenciais-e-boas-prÃ¡ticas)
- [ğŸ“Š Machine Learning](#-cenÃ¡rio-de-aplicaÃ§Ã£o-em-machine-learning)
- [ğŸš€ Deploy (MVP)](#-plano-de-deploy-mvp)
- [ğŸ”— Rotas](#-rotas-disponÃ­veis)
- [ğŸ“„ LicenÃ§a](#-licenÃ§a)

---

## ğŸ› ï¸ Tecnologias
- Python 3.12
- FastAPI
- SQLAlchemy
- **SQLite** (utilizado na Fase 1, pela facilidade de configuraÃ§Ã£o e aderÃªncia ao escopo do MVP)
- Alembic
- BeautifulSoup4

> âš ï¸ **ObservaÃ§Ã£o:**  
> O uso do SQLite foi uma decisÃ£o estratÃ©gica para acelerar o desenvolvimento e simplificar testes na Fase 1 do projeto.  
> **Nas prÃ³ximas fases**, o projeto estÃ¡ preparado para utilizar bancos de dados relacionais mais robustos (PostgreSQL, MariaDB, etc.), jÃ¡ integrados como serviÃ§os em containers (Docker), atendendo a requisitos de escalabilidade, concorrÃªncia e produÃ§Ã£o.

---

## ğŸ—‚ï¸ Estrutura do Projeto

Estrutura modular baseada em boas prÃ¡ticas de FastAPI e princÃ­pios de Clean Architecture:

- SeparaÃ§Ã£o clara por camadas (`api/`, `core/`, `models/`, `schemas/`, `crud/`)
- Roteamento versionado (`/api/v1/...`)

```
ğŸ“¦ API Embrapa - POS Tech MLE
â”œâ”€â”€ ğŸ“ app
â”‚   â”œâ”€â”€ ğŸ“ alembic                 # Migrations do banco de dados
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ ğŸ“ api                     # Camada de API
â”‚   â”‚   â””â”€â”€ ğŸ“ v1                  # VersÃ£o da API
â”‚   â”‚       â”œâ”€â”€ api.py             # Roteador principal
â”‚   â”‚       â”œâ”€â”€ ğŸ“ docs            # DocumentaÃ§Ã£o e responses
â”‚   â”‚       â”‚   â”œâ”€â”€ embrapa.py
â”‚   â”‚       â”‚   â””â”€â”€ responses.py
â”‚   â”‚       â”œâ”€â”€ ğŸ“ endpoints       # Endpoints da API
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â””â”€â”€ scraping.py
â”‚   â”‚       â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ ğŸ“ core                    # ConfiguraÃ§Ãµes e seguranÃ§a
â”‚   â”‚   â”œâ”€â”€ exceptions.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ middleware
â”‚   â”‚   â”‚   â””â”€â”€ docs_auth.py        # ProteÃ§Ã£o da doc Swagger
â”‚   â”‚   â””â”€â”€ security.py             # JWT, OAuth2, HTTPBasic, etc.
â”‚   â”œâ”€â”€ ğŸ“ crud                     # Camada de persistÃªncia
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ scraping.py
â”‚   â”œâ”€â”€ ğŸ“ db                       # SessÃ£o e base SQLAlchemy
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ session.py
â”‚   â”œâ”€â”€ ğŸ“ models                   # Modelos ORM
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ scraping.py
â”‚   â”œâ”€â”€ ğŸ“ schemas                  # Modelos Pydantic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ scraping.py
â”‚   â”œâ”€â”€ ğŸ“ scraping                 # MÃ³dulos de scraping
â”‚   â”‚   â”œâ”€â”€ bs4_scraper.py          # ServiÃ§o genÃ©rico
â”‚   â”‚   â”œâ”€â”€ exportacao.py           # CustomizaÃ§Ãµes especÃ­ficas
â”‚   â”‚   â”œâ”€â”€ importacao.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“ mocks                # Mocks HTML locais
â”‚   â”‚       â”œâ”€â”€ opt_02.html
â”‚   â”‚       â”œâ”€â”€ opt_03.html
â”‚   â”‚       â”œâ”€â”€ opt_04.html
â”‚   â”‚       â”œâ”€â”€ opt_05.html
â”‚   â”‚       â””â”€â”€ opt_06.html
â”‚   â””â”€â”€ ğŸ“ tests                    # Testes automatizados
â”‚       â””â”€â”€ __init__.py
|       â””â”€â”€ test_scraping.py
|
â”œâ”€â”€ ğŸš€ main.py                      # Ponto de entrada da aplicaÃ§Ã£o
â”œâ”€â”€ âš™ï¸ create_db.py                 # Script para criaÃ§Ã£o inicial do banco
â”œâ”€â”€ ğŸ—ƒï¸ embrapa.db                   # Banco SQLite
â”œâ”€â”€ ğŸ“„ poetry.lock                  # Lockfile de dependÃªncias
â”œâ”€â”€ ğŸ“„ pyproject.toml               # ConfiguraÃ§Ã£o do projeto (Poetry)
â”œâ”€â”€ ğŸ“„ requirements.txt             # Alternativa ao Poetry
â”œâ”€â”€ ğŸ“„ README.md                    # DocumentaÃ§Ã£o do projeto
```
---

## ğŸ“ˆ Diagramas do Projeto
Esta seÃ§Ã£o reÃºne os principais diagramas do projeto â€” **arquitetura macro**, **sequÃªncia**, **componentes**, **fluxos de alto nÃ­vel**, **fluxos detalhados** e **rotas** â€” que ilustram a arquitetura, funcionamento interno e endpoints da API Embrapa Uva e Vinho.  
Esses diagramas sÃ£o essenciais para onboarding de novos desenvolvedores, manutenÃ§Ã£o evolutiva e consulta tÃ©cnica rÃ¡pida.


> âš ï¸ ObservaÃ§Ã£o: Se vocÃª tiver problemas para visualizar os diagramas em Mermaid no GitHub, acesse a versÃ£o em imagem (PNG) disponÃ­vel nos links abaixo de cada diagrama.  
> UsuÃ¡rios do VS Code com suporte ao Mermaid podem visualizar normalmente em markdown.


### ğŸ”¹ 1. **Diagrama de arquitetura macro**
```mermaid
flowchart LR
    E[Cliente Externo / Sistema Integrado] <--> |REST API| B
    A[Embrapa Fonte de Dados] <--> |Web Scraping| B[API Embrapa]
    B <--> |Fallback/Cache| C[(Banco de Dados / CSV)]
    B <--> |AplicaÃ§Ãµes| D[Dashboards/ ML]
```
[ğŸ–¼ï¸ Ver diagrama em PNG](app/docs/diagramas/arquitetura_macro.png)

### ğŸ”¹ 2. **Diagrama fluxos de alto nÃ­vel**
```mermaid
flowchart TD
    Start([InÃ­cio])
    Req[Receber requisiÃ§Ã£o do cliente]
    Auth[AutenticaÃ§Ã£o vÃ¡lida?]
    Cache{Dados em cache?}
    Scraping[Executa scraping]
    Salva[Salva resultado no banco]
    Responde[Envia resposta ao cliente]
    Fim([Fim])

    Start --> Req --> Auth
    Auth -- NÃ£o --> Responde --> Fim
    Auth -- Sim --> Cache
    Cache -- Sim --> Responde --> Fim
    Cache -- NÃ£o --> Scraping --> Salva --> Responde --> Fim
```
[ğŸ–¼ï¸ Ver diagrama em PNG](app/docs/diagramas/fluxos_auto_nivel.png)


### ğŸ”¹ 3. **Diagrama de sequÃªncia**
```mermaid
sequenceDiagram
    participant Cliente
    participant API
    participant Schemas
    participant Core
    participant CRUD
    participant Scraping
    participant Models
    participant DB

    Note over Cliente,API: 1. Cliente faz requisiÃ§Ã£o para scraping

    Cliente->>API: RequisiÃ§Ã£o HTTP (GET /api/v1/scraping)
    API->>Core: Valida autenticaÃ§Ã£o/configuraÃ§Ã£o (opcional)
    API->>Schemas: Valida entrada (Pydantic)

    Note right of API: 2. API checa se dados jÃ¡ existem (cache)
    alt Dados jÃ¡ em cache?
        API->>CRUD: Consulta cache
        CRUD->>Models: Query (scraping cache)
        Models->>DB: Consulta banco
        DB-->>Models: Dados do cache
        Models-->>CRUD: Retorna dados
        CRUD-->>API: Retorna dados
        Note right of API: 3a. Se sim, API retorna dados imediatamente
        API-->>Cliente: Resposta (dados do cache)
    else NÃ£o existe cache
        Note right of API: 3b. Se nÃ£o, dispara serviÃ§o de scraping
        API->>Scraping: Chama serviÃ§o de scraping
        Scraping->>Site Embrapa: Coleta dados
        Site Embrapa-->>Scraping: Dados HTML
        Scraping->>CRUD: Persiste dados processados
        CRUD->>Models: Insert/Update
        Models->>DB: Grava no banco
        CRUD-->>API: Retorna dados processados
        API-->>Cliente: Resposta (dados atualizados)
    end

    Note over Cliente,API: 4. Cliente recebe os dados (cache ou novo scraping)

```
[ğŸ–¼ï¸ Ver diagrama em PNG](app/docs/diagramas/sequencia.png)

### ğŸ”¹ 4. **Diagrama de componentes**
```mermaid
flowchart TD
    subgraph API Embrapa
        MainPy[main.py]
        Alembic[Migrations alembic/]
        API[API api/v1/]
        Docs[DocumentaÃ§Ã£o api/v1/docs/]
        Endpoints[Endpoints api/v1/endpoints/]
        Core[Core Config e SeguranÃ§a]
        Middleware[Middleware docs_auth.py]
        CRUD[CRUD crud/]
        DB[DB SessÃ£o e Base]
        Models[Models models/]
        Schemas[Schemas schemas/]
        ScrapingService[Scraping Services scraping/]
        Mocks[Mocks HTML scraping/mocks/]
        Tests[Tests tests/]
        Cliente[Cliente]
        DBStorage[(Banco de Dados)]
    end


    Cliente -->|RequisiÃ§Ã£o| API
    MainPy -->|Entrypoint| API
    API -->|Inclui| Docs
    API -->|Usa| Endpoints
    API -->|Usa config/seguranÃ§a| Core
    Core -->|Middleware| Middleware
    API -->|Usa| CRUD
    API -->|Usa| Schemas
    API -->|Usa| Models
    API -->|Dispara| ScrapingService
    ScrapingService -->|Usa| Mocks
    CRUD -->|Persiste| DB
    CRUD -->|Usa| Models
    Models -->|Definem ORM| DB
    DB -->|Session/engine| DBStorage
    Tests -->|Testa| API
    Tests -->|Testa| ScrapingService
    Alembic -->|Migra| DBStorage
```
[ğŸ–¼ï¸ Ver diagrama em PNG](app/docs/diagramas/componentes.png)


### ğŸ”¹ 5. **Diagrama fluxos detalhados**
```mermaid
flowchart TD
    Cliente -->|1 RequisiÃ§Ã£o HTTP| APIv1
    APIv1 -->|2 ValidaÃ§Ã£o e parsing| Schemas
    APIv1 -->|3 AutenticaÃ§Ã£o e seguranÃ§a| Core
    APIv1 -->|4 Encaminha para endpoint| Endpoints
    Endpoints -->|5 Chama camada de persistÃªncia| CRUD
    CRUD -->|6 Usa modelos ORM| Models
    CRUD -->|7 Persiste ou busca dados| DB
    APIv1 -->|8 Retorna resposta| Cliente
    APIv1 -->|9 Solicita scraping| ScrapingService
    ScrapingService -->|10 Realiza scraping e retorna dados| CRUD
    Alembic -. |11 Migrations| .-> DB

    subgraph API
        APIv1
        Endpoints
    end

    subgraph Banco
        DB
        Alembic
    end

    subgraph Dados
        Models
        Schemas
        CRUD
    end

    subgraph Servicos
        ScrapingService
        Core
    end
```
[ğŸ–¼ï¸ Ver diagrama em PNG](app/docs/diagramas/fluxos_detalhe.png)

### ğŸ”¹ 6. **Diagrama de rotas**
```mermaid
flowchart TD
    subgraph API Rotas
        Cliente -->|POST /producao| Producao
        Producao -->|Consulta/Salva| DB
        Producao -->|Executa scraping| Scraping

        Cliente -->|POST /processamento| Processamento
        Processamento -->|Consulta/Salva| DB
        Processamento -->|Executa scraping| Scraping

        Cliente -->|POST /comercializacao| Comercializacao
        Comercializacao -->|Consulta/Salva| DB
        Comercializacao -->|Executa scraping| Scraping

        Cliente -->|POST /importacao| Importacao
        Importacao -->|Consulta/Salva| DB
        Importacao -->|Executa scraping| Scraping

        Cliente -->|POST /exportacao| Exportacao
        Exportacao -->|Consulta/Salva| DB
        Exportacao -->|Executa scraping| Scraping

        Scraping -->|Retorna dados| DB

    
        Producao
        Processamento
        Comercializacao
        Importacao
        Exportacao
    end
```
[ğŸ–¼ï¸ Ver diagrama em PNG](app/docs/diagramas/rotas.png)


### ğŸ—ºï¸ Fluxo Detalhado da API

| Etapa | DescriÃ§Ã£o                                        | Arquivo                            |
| :---- | :----------------------------------------------- | :--------------------------------- |
| 1     | API recebe requisiÃ§Ã£o                            | `app/main.py`                      |
| 2     | RequisiÃ§Ã£o roteada pela API v1                   | `app/api/v1/api.py`                |
| 3     | Endpoint `embrapa_producao()` ou outro Ã© chamado | `app/api/v1/endpoints/scraping.py` |
| 4     | ServiÃ§o de scraping Ã© ativado                    | `app/scraping/bs4_scraper.py`      |
| 5     | Parser executado com BeautifulSoup               | `app/scraping/bs4_scraper.py`      |
| 6     | Verifica se dados existem no banco               | `app/crud/scraping.py`             |
| 7     | Se nÃ£o existir, salva dados no banco             | `app/crud/scraping.py`             |
| 8     | Formata dados com Pydantic                       | `app/schemas/scraping.py`          |
| 9     | Resposta JSON enviada ao cliente                 | `-`                            |


---

## âš™ï¸ InstalaÃ§Ã£o e ExecuÃ§Ã£o

### âœ… **OpÃ§Ã£o 1: Rodar localmente com Poetry**

```bash
git clone https://github.com/Isabelle-Fideles/tech-challenge-embrapa.git
cd tech-challenge-embrapa/fase1
poetry install
poetry shell
uvicorn app.main:app --reload --port 8000
```

### âœ… **OpÃ§Ã£o 2: Rodar com Docker**

```bash
git clone https://github.com/Isabelle-Fideles/tech-challenge-embrapa.git
cd tech-challenge-embrapa/fase1
docker compose up -d --build
```
Depois, acesse:
- â¡ï¸ http://localhost:8000 â†’ API funcionando.
- â¡ï¸ http://localhost:8000/docs â†’ Swagger UI.

#### âœ… **Para parar:**

```bash
docker compose down
```

#### âœ… **Outros comandos Ãºteis::**

- Ver logs:
```bash
docker compose logs -f api
```
- Limpar imagens/parar containers:
```bash
docker system prune -af --volumes
```
---

## ğŸ” AutenticaÃ§Ã£o
AutenticaÃ§Ã£o via **HTTPBasic** foi implementada (FASE 1) como proteÃ§Ã£o opcional da documentaÃ§Ã£o (Swagger):
- Middleware: app/core/middleware/docs_auth.py (**DESABILITADO**)
- Rota protegida: /docs (**HABILITADO**)

---

## ğŸ”‘ Credenciais de Acesso (Demo)

Para acessar a documentaÃ§Ã£o Swagger ou utilizar os endpoints protegidos, utilize as seguintes credenciais de teste:

- **UsuÃ¡rio:** `admin`
- **Senha:** `!@#$Fiap2025`

***Essas credenciais sÃ£o exclusivas para avaliaÃ§Ã£o e uso em ambiente de testes.***
> âš ï¸ **Importante:** NÃ£o utilize estas credenciais em produÃ§Ã£o.

---

## ğŸ§ª Testes

#### âœ… Como executar os testes

Execute na raiz do projeto:

```bash
PYTHONPATH=. pytest app/tests -v
```

Ou, alternativamente:

```bash
python -m pytest app/tests -v
```



#### âœ… PrÃ©-requisitos

- DependÃªncias instaladas (`poetry install` ou `pip install -r requirements.txt`)
- Estar na raiz do projeto (`/fase1` ou similar)
- Ter os arquivos de mock HTML na pasta:

```
app/scraping/mocks/
â”œâ”€â”€ opt_02.html
â”œâ”€â”€ opt_03.html
â”œâ”€â”€ opt_04.html
â”œâ”€â”€ opt_05.html
â””â”€â”€ opt_06.html
```



#### âœ… O que Ã© testado

- ğŸ”— **`build_embrapa_url`** â€” ConstruÃ§Ã£o correta das URLs da Embrapa.
- ğŸŒ **`fetch_page_content`** â€” Download de conteÃºdo HTML, com tratamento de falhas simuladas (timeouts, erros de conexÃ£o, indisponibilidade).
- ğŸ“„ **`parse_table`** e **`parse_import_export_table`** â€” Parsing de HTML para JSON estruturado.
- ğŸ”¥ **`scrape_embrapa`** â€” Scraping completo, verificando se dados existem, salvamento e resposta formatada.
- âš ï¸ **ExceÃ§Ãµes** â€” Tratamento de erros como `ExternalServiceUnavailableException` e `EmbrapaDataNotFoundException`.



#### ğŸš© ObservaÃ§Ãµes importantes

Se ocorrer o erro:

```plaintext
ModuleNotFoundError: No module named 'app'
```

Garanta que vocÃª estÃ¡ executando com o parÃ¢metro:

```bash
PYTHONPATH=. pytest app/tests
```

Ou usando o modo mÃ³dulo:

```bash
python -m pytest app/tests
```

Isso Ã© necessÃ¡rio porque o Python precisa reconhecer o diretÃ³rio `app/` como parte do caminho de importaÃ§Ã£o.



#### ğŸ§  OrganizaÃ§Ã£o dos testes

```
app/tests/
â””â”€â”€ test_scraping.py
```



#### ğŸ† Exemplo de saÃ­da esperada

```plaintext
================================= test session starts ===============================
collected 12 items

app/tests/test_scraping.py ............                                       [100%]

============================== 12 passed in 0.42s ==============================
```



#### ğŸš© Dicas profissionais

- âœ… Recomenda-se rodar os testes sempre antes de qualquer commit.
- âœ… Para automaÃ§Ã£o, considere incluir no pipeline de CI/CD (`GitHub Actions`, `GitLab CI`, `Render`, etc.).

---

## â­ Diferenciais e Boas PrÃ¡ticas

- Fallback automÃ¡tico para cache local caso o site da Embrapa esteja fora do ar, garantindo alta disponibilidade da API.
- Projeto altamente modularizado, facilitando manutenÃ§Ã£o, testes e expansÃ£o.
- AutenticaÃ§Ã£o bÃ¡sica implementada para documentaÃ§Ã£o via Swagger.
- DocumentaÃ§Ã£o completa: Swagger UI, diagramas em Mermaid e PNG.
- Pronto para integraÃ§Ã£o futura com dashboards (ex: Power BI, Streamlit) e projetos de Machine Learning.
- Uso de variÃ¡veis sensÃ­veis centralizadas em `.env` (python-dotenv), aumentando a seguranÃ§a.
- Commits claros e organizados, seguindo boas prÃ¡ticas de versionamento.
- Filtros nos endpoints, evitando sobrecarga de dados e melhorando performance.
- Testes automatizados cobrindo scraping, parsing e tratamento de exceÃ§Ãµes.

## âš¡ OtimizaÃ§Ã£o e Ãndices no Banco de Dados

Todas as tabelas do projeto possuem Ã­ndices explÃ­citos nas colunas mais consultadas, conforme recomendado em boas prÃ¡ticas.  
Foram criados Ã­ndices em:

- **ano**
- **opcao**
- **subopcao** (quando aplicÃ¡vel)
- **id** (chave primÃ¡ria, Ã­ndice automÃ¡tico)

Esses Ã­ndices garantem alta performance nas consultas realizadas pela API, principalmente nos filtros por ano, tipo de dado (opcao) e categoria (subopcao).  
A modelagem foi pensada para suportar um grande volume de dados sem perda de eficiÃªncia.

---

## ğŸ“Š CenÃ¡rio de AplicaÃ§Ã£o em Machine Learning
Os dados coletados poderÃ£o ser utilizados para:
- **PrediÃ§Ã£o de produÃ§Ã£o de uvas por estado**
- **ClassificaÃ§Ã£o de tipos de vinho por perfil de exportaÃ§Ã£o**
- **AnÃ¡lise de tendÃªncias na comercializaÃ§Ã£o e importaÃ§Ã£o**

---

## ğŸš€ Plano de Deploy (MVP)

O projeto pode ser facilmente publicado em:

- Railway (deploy contÃ­nuo via Git)
- Render.com
- **Docker + Uvicorn em VPS (Ex: EC2)**

---

## ğŸ”— Rotas disponÃ­veis

- /api/v1/embrapa/producao
- /api/v1/embrapa/processamento
- /api/v1/embrapa/comercializacao
- /api/v1/embrapa/importacao
- /api/v1/embrapa/exportacao

## ğŸ“„ LicenÃ§a
MIT License.
